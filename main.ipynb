{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import tf_text to load the ops used by the tokenizer saved model\n",
    "import tensorflow_text  # pylint: disable=unused-import"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "logging.getLogger('tensorflow').setLevel(logging.ERROR) # suppress warnings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-05 22:16:01.293927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-05 22:16:01.352057: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-04-05 22:16:01.352077: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-04-05 22:16:01.352380: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True, as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Yields pairs of text examples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
      "mas e se estes fatores fossem ativos ?\n",
      "mas eles não tinham a curiosidade de me testar .\n",
      "\n",
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "but what if it were active ?\n",
      "but they did n't test for curiosity .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-05 22:16:08.455106: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
    "  for pt in pt_examples.numpy():\n",
    "    print(pt.decode('utf-8'))\n",
    "\n",
    "  print()\n",
    "\n",
    "  for en in en_examples.numpy():\n",
    "    print(en.decode('utf-8'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/models/ted_hrlr_translate_pt_en_converter.zip\n",
      "188416/184801 [==============================] - 1s 3us/step\n",
      "196608/184801 [===============================] - 1s 3us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "'./ted_hrlr_translate_pt_en_converter.zip'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['HTTP_PROXY']='http://127.0.0.1:1080'\n",
    "os.environ['HTTPS_PROXY']='http://127.0.0.1:1080'\n",
    "model_name = 'ted_hrlr_translate_pt_en_converter'\n",
    "tf.keras.utils.get_file(\n",
    "    f'{model_name}.zip',\n",
    "    f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',\n",
    "    cache_dir='.', cache_subdir='', extract=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "tokenizers = tf.saved_model.load(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "['detokenize',\n 'get_reserved_tokens',\n 'get_vocab_path',\n 'get_vocab_size',\n 'lookup',\n 'tokenize',\n 'tokenizer',\n 'vocab']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in dir(tokenizers.en) if not item.startswith('_')]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The \"tokenize\" method converts a batch of strings to a padded-batch pf token IDs. This method splits punctuation, lowercases and unicodenormalizes the input before tokenizing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "but what if it were active ?\n",
      "but they did n't test for curiosity .\n"
     ]
    }
   ],
   "source": [
    "for en in en_examples.numpy():\n",
    "    print(en.decode('utf-8'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 72, 117, 79, 1259, 1491, 2362, 13, 79, 150, 184, 311, 71, 103, 2308, 74, 2679, 13, 148, 80, 55, 4840, 1434, 2423, 540, 15, 3]\n",
      "[2, 87, 90, 107, 76, 129, 1852, 30, 3]\n",
      "[2, 87, 83, 149, 50, 9, 56, 664, 85, 2512, 15, 3]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizers.en.tokenize(en_examples)\n",
    "\n",
    "for row in encoded.to_list():\n",
    "    print(row)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Detokenize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "but what if it were active ?\n",
      "but they did n ' t test for curiosity .\n"
     ]
    }
   ],
   "source": [
    "round_trip = tokenizers.en.detokenize(encoded)\n",
    "\n",
    "for line in round_trip.numpy():\n",
    "    print(line.decode('utf-8'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The lower level lookup method converts from token-IDs to token text\n",
    "Here you can see the subword aspect of the tokenizers. The word \"searchability\" is decomposed into \"search ##ability\" and the word \"serendipity\" into \"s ##ere ##nd ##ip ##ity\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[b'[START]', b'and', b'when', b'you', b'improve', b'search', b'##ability',\n  b',', b'you', b'actually', b'take', b'away', b'the', b'one', b'advantage',\n  b'of', b'print', b',', b'which', b'is', b's', b'##ere', b'##nd', b'##ip',\n  b'##ity', b'.', b'[END]']                                                 ,\n [b'[START]', b'but', b'what', b'if', b'it', b'were', b'active', b'?',\n  b'[END]']                                                           ,\n [b'[START]', b'but', b'they', b'did', b'n', b\"'\", b't', b'test', b'for',\n  b'curiosity', b'.', b'[END]']                                          ]>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizers.en.lookup(encoded)\n",
    "tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}